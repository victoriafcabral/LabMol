{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import functions\n",
    "import sys,os\n",
    "import glob\n",
    "try: \n",
    "    if(cwd is not None):\n",
    "        from functions.utils_curation import *\n",
    "except:\n",
    "    %cd ..\n",
    "    cwd = os.getcwd()\n",
    "    sys.path.insert(0,cwd)\n",
    "    from functions.utils_curation import *\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Define file name\n",
    "#path to file IS NOT NEEDED\n",
    "#fname = 'single protein format - (IC50only) precuration.csv'\n",
    "\n",
    "#summary\n",
    "standardiseLoc = 'standardise_summary.csv'\n",
    "binaryDupRemov = 'binaryDup_summary.csv'\n",
    "regressDupRemov = 'regressionDup_summary.csv'\n",
    "multiclasDupRemov = 'multiclass_summary.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DATA PREPARATION AND STANDARDISATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load and merge datasets\n",
    "ds1 = check_extention('cell based format - ic50_CHO.csv', 1)\n",
    "ds2 = check_extention('cell based format - ic50_HEK.csv', 1)\n",
    "ds3 = check_extention('single protein format - (IC50only) precuration.csv', 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds1 = ds1[['Molecule ChEMBL ID', 'Smiles', 'Standard Type', 'Standard Value', 'Standard Units', 'Standard Relation']]\n",
    "ds2 = ds2[['Molecule ChEMBL ID', 'Smiles', 'Standard Type', 'Standard Value', 'Standard Units', 'Standard Relation']]\n",
    "ds3 = ds3[['Molecule ChEMBL ID', 'Smiles', 'Standard Type', 'Standard Value', 'Standard Units', 'Standard Relation']]\n",
    "\n",
    "dflist = [ds1, ds2, ds3]\n",
    "#concat\n",
    "df0 = pd.concat(dflist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load_data\n",
    "#df0 = check_extention(fname, 1)\n",
    "datasuma = [['initial', len(df0)]]\n",
    "datasuma = pd.DataFrame(datasuma, columns=['process', 'value'])\n",
    "datasuma.to_csv(f'./data/data_summary/{standardiseLoc}', index=False)\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#drop missing values\n",
    "df0 = df0.dropna(subset=['Standard Value'])\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{standardiseLoc}')\n",
    "new_row = ['missing activity removed', int(datasuma['value'][0])-len(df0)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df0)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{standardiseLoc}', index=False)\n",
    "\n",
    "df0 = df0.dropna(subset=['Standard Units'])\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{standardiseLoc}')\n",
    "new_row = ['missing units removed', int(datasuma['value'][2])-len(df0)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df0)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{standardiseLoc}', index=False)\n",
    "\n",
    "\n",
    "df0 = df0.reset_index()\n",
    "df0 = df0.drop(columns = 'index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if every activity data is the same\n",
    "df0.groupby('Standard Type').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check if every activity data is the same\n",
    "df0.groupby('Standard Units').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted activity data\n",
    "diffact = []\n",
    "DropList = []\n",
    "for index, activity in enumerate(df0['Standard Units']):\n",
    "    if str(activity).lower() == 'nm':\n",
    "        pass\n",
    "    else:\n",
    "        diffact.append(df0.iloc[[index]])\n",
    "        DropList.append(index)\n",
    "\n",
    "if len(diffact) == 0:\n",
    "    pass\n",
    "else:\n",
    "    df0 = df0.drop(DropList, errors=\"ignore\")\n",
    "    diffact = pd.concat(diffact)\n",
    "    diffact.to_csv(f'{errorverbose}different_activity.csv')\n",
    "    df0 = df0.reset_index()\n",
    "    df0 = df0.drop(columns = 'index')\n",
    "    datasuma = pd.read_csv(f'./data/data_summary/{standardiseLoc}')\n",
    "    new_row = ['different activity removed', int(datasuma['value'][4])-len(df0)]\n",
    "    datasuma.loc[len(datasuma.index)] = new_row\n",
    "    datasuma.to_csv(f'./data/data_summary/{standardiseLoc}', index=False)\n",
    "    new_row = ['total', len(df0)]\n",
    "    datasuma.loc[len(datasuma.index)] = new_row\n",
    "    datasuma.to_csv(f'./data/data_summary/{standardiseLoc}', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convert IC50 to pIC50\n",
    "pIC50 = []\n",
    "for value in df0['Standard Value']:\n",
    "    value = value / 1000\n",
    "    pIC50.append(-(math.log10(value*10**-6)))\n",
    "\n",
    "df0['pIC50 (uM)'] = pIC50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove unwanted columns\n",
    "dropList = ['Standard Type', 'Standard Units']\n",
    "df0 = df0.drop(columns = dropList)\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rename columns if needed\n",
    "df0.rename(columns = {'Molecule ChEMBL ID':'ID', 'Smiles':'SMILES', 'Standard Relation':'Relation', 'Standard Value': 'IC50 (uM)'}, inplace = True)\n",
    "df0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#structural curation\n",
    "curated_dataset = curate(df0, errorverbose, save_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUPLICATE REMOVAL BINARY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load standardised data\n",
    "fname = 'standardised_but_no_duplicates_removed.csv'\n",
    "df1 = check_extention(fname, 2)\n",
    "\n",
    "datasuma = [['initial', len(df1)]]\n",
    "datasuma = pd.DataFrame(datasuma, columns=['process', 'value'])\n",
    "datasuma.to_csv(f'./data/data_summary/{binaryDupRemov}', index=False)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check types of relation\n",
    "df1.groupby('Relation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = relationTreat(dataset = df1, relationcolumn = 'Relation', activitycolumn = 'IC50 (uM)', threshold = 10, curationtype = 'binary', errorverbose = errorverbose, summary=binaryDupRemov)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define threshold\n",
    "outcome = [1 if activity > 5 else 0 for activity in df1['pIC50 (uM)']]\n",
    "df1['Outcome'] = outcome\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group duplicates based on inchikey\n",
    "df1_agg = group(df1, ['pIC50 (uM)', 'ID', 'SMILES', 'Outcome'])\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{binaryDupRemov}')\n",
    "new_row = ['duplicates total', len(df1) - len(df1_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df1_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{binaryDupRemov}', index=False)\n",
    "df1_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates with stddev > 0\n",
    "df1_agg = dupRemovalClassification(df1_agg, errorverbose, 'Outcome', 'binary')\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{binaryDupRemov}')\n",
    "new_row = ['discordant duplicates', int(datasuma['value'][9]) - len(df1_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df1_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{binaryDupRemov}', index=False)\n",
    "df1_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df1 = removeListedValues(df1_agg)\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save curated data\n",
    "df1.to_csv(f'{save_data}curated_binary.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUPLICATE REMOVAL REGRESSION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load standardised data\n",
    "fname = 'standardised_but_no_duplicates_removed.csv'\n",
    "df2 = check_extention(fname, 2)\n",
    "datasuma = [['initial', len(df2)]]\n",
    "datasuma = pd.DataFrame(datasuma, columns=['process', 'value'])\n",
    "datasuma.to_csv(f'./data/data_summary/{regressDupRemov}', index=False)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove relations != \"=\"\n",
    "\n",
    "index_drop = []\n",
    "for index, relation in enumerate(df2['Relation']):\n",
    "    if relation != \"'='\":\n",
    "        index_drop.append(index)\n",
    "    else:\n",
    "        pass\n",
    "\n",
    "if len(index_drop) == 0:\n",
    "    pass\n",
    "else:\n",
    "    mask = df2.iloc[index_drop]\n",
    "    df2 = df2.drop(index_drop, errors=\"ignore\")\n",
    "    mask.to_csv(\"{}relationsRemoved_regression.csv\".format(errorverbose), sep=',', header=True, index=False)\n",
    "\n",
    "df2 = df2.reset_index()\n",
    "df2 = df2.drop(columns = 'index')\n",
    "\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{regressDupRemov}')\n",
    "new_row = [\"removed relations != '='\", int(datasuma['value'][0]) - len(df2)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df2)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{regressDupRemov}', index=False)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check relations\n",
    "df2.groupby('Relation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Drop relations column\n",
    "df2 = df2.drop(columns = 'Relation')\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group duplicates based on inchikey\n",
    "df2_agg = group(df2, ['pIC50 (uM)', 'ID', 'SMILES'])\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{regressDupRemov}')\n",
    "new_row = ['duplicates total', len(df2) - len(df2_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df2_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{regressDupRemov}', index=False)\n",
    "df2_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2_agg = dupRemovalRegression(df2_agg, errorverbose, 'pIC50 (uM)', 0.2)\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{regressDupRemov}')\n",
    "new_row = ['discordant duplicates', int(datasuma['value'][4]) - len(df2_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df2_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{regressDupRemov}', index=False)\n",
    "df2_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df2 = removeListedValues(df2_agg)\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save curated data\n",
    "df2.to_csv(f'{save_data}curated_regression.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DUPLICATE REMOVAL MULTICLASS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#load standardised data\n",
    "fname = 'standardised_but_no_duplicates_removed.csv'\n",
    "df3 = check_extention(fname, 2)\n",
    "datasuma = [['initial', len(df3)]]\n",
    "datasuma = pd.DataFrame(datasuma, columns=['process', 'value'])\n",
    "datasuma.to_csv(f'./data/data_summary/{multiclasDupRemov}', index=False)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#check types of relation\n",
    "df3.groupby('Relation').size()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = relationTreat(dataset = df3, relationcolumn = 'Relation', activitycolumn = 'IC50 (uM)', threshold = 10, curationtype = 'multiclass', errorverbose = errorverbose, summary=multiclasDupRemov)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#define threshold\n",
    "outcome = []\n",
    "\n",
    "for activity in df3['pIC50 (uM)']:\n",
    "    #nonblocker\n",
    "    if activity < 4.5:\n",
    "        outcome.append(0)\n",
    "    elif activity < 5 and activity >= 4.5:\n",
    "        outcome.append(1)\n",
    "    elif activity >= 5 and activity < 6:\n",
    "        outcome.append(2)\n",
    "    else:\n",
    "        outcome.append(3)\n",
    "df3['Outcome'] = outcome\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#group duplicates based on inchikey\n",
    "df3_agg = group(df3, ['pIC50 (uM)', 'ID', 'SMILES', 'Outcome'])\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{multiclasDupRemov}')\n",
    "new_row = ['duplicates total', len(df3) - len(df3_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df3_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{multiclasDupRemov}', index=False)\n",
    "df3_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove duplicates with stddev > 0\n",
    "df3_agg = dupRemovalClassification(df3_agg, errorverbose, 'Outcome', 'multiclass')\n",
    "datasuma = pd.read_csv(f'./data/data_summary/{multiclasDupRemov}')\n",
    "new_row = ['discordant duplicates', int(datasuma['value'][4]) - len(df3_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "new_row = ['total', len(df3_agg)]\n",
    "datasuma.loc[len(datasuma.index)] = new_row\n",
    "datasuma.to_csv(f'./data/data_summary/{multiclasDupRemov}', index=False)\n",
    "df3_agg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df3 = removeListedValues(df3_agg)\n",
    "df3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#save curated data\n",
    "df3.to_csv(f'{save_data}curated_multiclass.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 ('chem_pipeline')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a59e22669eba1dc6e4d930302cbb600e9d71d81892f6be3c25d1b723809ef057"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
